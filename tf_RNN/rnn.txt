循环神经网络综述
RNN源自于1982年Saratha Sathasivarn提出的霍普菲尔德网络。全连接存在参数太多，无法利用数据中时间序列信息等问题
主要用途：处理和预测序列数据
循环神经网络的隐藏层之间的节点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出
由于一个模块中的运算和变量在不同时刻是相同的，因此循环神经网络理论上可以被看作是同一神经网络结构被无限复制的结果。正如CNN在不同的空间位置共享参数，循环神经网络在不同时间位置共享参数，从而使用有限的参数处理任意长度的序列
沿时间反向传播（	Back-Propagation Through Time）
输入可以是一个序列数据，输出可以是对序列中下一个时刻的预测，也可以是对当前时刻信息的处理结果（比如语音识别结果）。循环神经网络要求每一个时刻都有一个输入，但是不一定每个时刻都需要有输出
循环神经网络中的状态是通过一个向量来表示的，这个向量的维度也被称为RNN的隐藏层大小。
输入样例可以是当前时刻的数值(时间序列数据)，可以是当前单词对应的词向量(语言模型)
假设输入向量的维度为x,隐藏状态的维度为n，那么循环体的全连接层神经网络的输入大小为n+x。
循环神经网络唯一的区别在于因为它每个时刻都有一个输出，所以循环神经网络的总损失为所有时刻上的损失函数之和