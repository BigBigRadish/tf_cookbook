#卷积神经网络
'''
CNN简介：卷积神经网络最初是为了解决图像识别等问题设计的，现在已经不仅仅是图像和视频，也可以用于时间序列信号，比如音频信号、文本数据等。在早期的图像研究中，最大的挑战如何组织特征，因为图像数据不像其他类型的数据那样可以通过人工理解来提取特征。在深度学习出现之前，我们必须借助SIFT，HOG等算法提取具有良好区分性的特征（SIFT等可以参考我的博文OpenCV图像处理），再集合SVM等机器学习算法进行图像识别。SIFT对一定程度的缩放、平移、旋转、视角改变、亮度调整等畸变，都具有不变性，是当时最重要的图像特征提取方法之一。可以说，在之前只能依靠SIFT等特征提取算法才能勉强进行可靠的图像识别。
然而，SIFT这类算法体征提取还是有局限性的，一般错误率在26%以上。CNN提取的特征可以达到更好的效果，同时它不需要将特征提取和分类训练两个过程分开，它在训练时就自动提取了最有效的特征。CNN可以直接使用图像的原始像素作为输入，而不必使用SIFT等算法提取特征，减轻了使用传统算法如SVM时必须要做的大量重复工作。和SIFT的等算法相似，CNN训练的模型同样对缩放、平移、旋转等畸变具有不变性，有很强的泛化性。CNN的最大特点在于卷积的权值共享结构，可以大幅度减少神经网络的参数量，防止过拟合的同时又降低了神经网络的复杂度。

一般的卷积神经网络有多个卷积层构成，每个卷积层中通常会进行如下几个操作。
（1）图像通过多个不同的卷积核的滤波，并加偏置，提取局部特征特征，每一个卷积核会映射出一个新的2D图像。
（2）将前面卷积的滤波器输出结果，进行非线性的激活函数处理，目前最常用的是使用relu函数，而以前sigmoid函数用的比较多。
（3）对激活函数的结果再进行池化操作（即降采样，比如2x2的图片降为1X1的图片），目前一般是使用最大池化，保留最显著特征，提升模型的畸变容忍能力。

全连接NN：每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测结果。
参数个数：（前层X后层+后层） 
例：在前面5x5x1 的图片周围进行全零填充，可使输出图片仍保持 5x5x1 的维度。这个全零填充的过程叫做 padding。

输出数据体的尺寸=（W-F+2P）/S+1
W：输入数据体尺寸，
F：卷积层神经元感知域，
S：步长，
P：0填充的数量

神经网络训练过程中，为了减少过多参数常使用dropout的方法，将一部分神经元按照一定概率从神经网络中舍弃。这种舍弃是临时性的，仅在训练时舍弃一些神经元，在使用神经网络时，会把所有的神经元恢复到神经网络中。dropout有效减少过拟合。
TensorFlow提供的dropout的函数：用tf.nn.dropout函数。第一个参数连接上一层 的输出，第二个参数给出神经元舍弃的概率。
在实际应用，常常在前向传播构建神经网络时使用dropout来减少过拟合加快模型想训练速度。
dropout一般会放到全连接网络中。如果在训练参数的过程中，输出=tf.nn.dropout（上层输出，暂时舍弃神经元的概率），这样就有指定概率的神经元随机置0，置0的神经元不参加当前轮的参数优化。
卷积NN：借助卷积核kernel提取特征后，送入全连接网络。
卷积神经网络可以认为由两部分组成，一部分是对输入图片进行特征提取，另一部分就是全连接网络，只不过喂入全连接网络的图片不再是原始图片，而是经过若干次卷积、激活和池化的特征信息。
卷积网络从诞生到现在，已经出现了许多经典网络结构，比如Lenet-5，Alenet，VGGnet，Googlenet，ResNet等。每一种网络结构都是以卷积、激活、池化、全连接这四种操作作为基础。
'''
